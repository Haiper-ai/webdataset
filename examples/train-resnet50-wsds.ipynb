{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import wsds\n",
    "reload(wsds)\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def enumerate_report(seq, delta, growth=1.0):\n",
    "    last = 0\n",
    "    count = 0\n",
    "    for count, item in enumerate(seq):\n",
    "        now = time.time()\n",
    "        if now - last > delta:\n",
    "            last = now\n",
    "            yield count, item, True\n",
    "        else:\n",
    "            yield count, item, False\n",
    "        delta *= growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "epochs = 3\n",
    "max_steps = 100000\n",
    "batch_size = 32\n",
    "bucket = \"https://storage.googleapis.com/webdataset/fake-imagenet/\"\n",
    "num_workers = 4\n",
    "cache_dir = \"./_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The standard TorchVision transformations.\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_val = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # All the decoding and data augmentation is done in the make_sample function.\n",
    "\n",
    "def make_sample(sample, val=False):\n",
    "    # decode the sample in place\n",
    "    wsds.decode_basic(sample)\n",
    "    wsds.decode_images_to_pil(sample)\n",
    "\n",
    "    # extract the image and label\n",
    "    image = sample[\".jpg\"]\n",
    "    label = sample[\".cls\"]\n",
    "\n",
    "    # apply the transformations\n",
    "    if val:\n",
    "        return transform_val(image), label\n",
    "    else:\n",
    "        return transform_train(image), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.3994,  0.3309,  0.4337,  ...,  0.1597,  0.2624,  0.3481],\n",
       "          [ 0.4851,  0.4679,  0.5022,  ...,  0.4508,  0.5707,  0.6221],\n",
       "          [ 0.4166,  0.4679,  0.4679,  ...,  0.3994,  0.3994,  0.5707],\n",
       "          ...,\n",
       "          [-0.6794, -0.4568, -0.5596,  ..., -0.6281, -0.6452, -0.6623],\n",
       "          [-0.6965, -0.4911, -0.6281,  ..., -0.4911, -0.6281, -0.5767],\n",
       "          [-0.7993, -1.0562, -1.1760,  ..., -0.6452, -0.5424, -0.6109]],\n",
       " \n",
       "         [[ 0.0476,  0.0126,  0.1001,  ..., -0.0924, -0.0224,  0.0651],\n",
       "          [ 0.1702,  0.1527,  0.1527,  ...,  0.2227,  0.2927,  0.3627],\n",
       "          [ 0.0826,  0.1352,  0.1352,  ...,  0.1702,  0.1001,  0.3102],\n",
       "          ...,\n",
       "          [-0.8277, -0.6001, -0.7402,  ..., -0.8452, -0.8452, -0.8627],\n",
       "          [-0.8803, -0.6702, -0.7927,  ..., -0.6877, -0.8277, -0.7752],\n",
       "          [-0.9678, -1.2304, -1.3354,  ..., -0.8452, -0.7577, -0.8102]],\n",
       " \n",
       "         [[ 0.0431,  0.0082,  0.0779,  ...,  0.0431,  0.1128,  0.1999],\n",
       "          [ 0.1651,  0.1302,  0.1476,  ...,  0.3219,  0.4091,  0.4439],\n",
       "          [ 0.0605,  0.0953,  0.0953,  ...,  0.2522,  0.2173,  0.3568],\n",
       "          ...,\n",
       "          [-0.6890, -0.4450, -0.5844,  ..., -0.6890, -0.7064, -0.7064],\n",
       "          [-0.7064, -0.5147, -0.6541,  ..., -0.5321, -0.6715, -0.6193],\n",
       "          [-0.7587, -1.0201, -1.1770,  ..., -0.6541, -0.5670, -0.6367]]]),\n",
       " 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec = f\"\"\"---\n",
    "train:\n",
    "    sequential:\n",
    "        shards: {bucket}imagenet-train.json\n",
    "        shuffle_size: 10000\n",
    "        cache_dir: {cache_dir}\n",
    "        keep_downloaded: true\n",
    "val:\n",
    "    sequential:\n",
    "        shards: {bucket}imagenet-val.json\n",
    "        cache_dir: {cache_dir}\n",
    "        keep_downloaded: true\n",
    "\"\"\"\n",
    "\n",
    "trainset = wsds.SequentialDataset(spec, transformations=make_sample)\n",
    "next(iter(trainset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.1932, -1.1932, -1.1932,  ...,  0.3138,  0.3481,  0.3823],\n",
       "          [-1.1932, -1.1589, -1.1760,  ...,  0.2796,  0.2796,  0.3309],\n",
       "          [-1.1589, -1.1418, -1.1418,  ...,  0.2282,  0.2453,  0.3138],\n",
       "          ...,\n",
       "          [-0.9534, -1.0904, -1.2274,  ...,  0.8961,  0.8789,  0.7591],\n",
       "          [-0.7650, -0.8849, -0.9705,  ...,  0.7419,  0.7248,  0.7762],\n",
       "          [-0.7993, -0.8335, -0.8507,  ...,  0.7762,  0.7591,  0.7248]],\n",
       " \n",
       "         [[-0.8978, -0.9328, -0.8978,  ...,  1.4307,  1.4657,  1.5007],\n",
       "          [-0.9153, -0.8978, -0.8978,  ...,  1.3957,  1.4307,  1.4832],\n",
       "          [-0.8978, -0.8803, -0.8803,  ...,  1.3606,  1.3957,  1.4657],\n",
       "          ...,\n",
       "          [-0.3025, -0.4426, -0.5826,  ...,  1.6933,  1.6758,  1.6057],\n",
       "          [ 0.1176, -0.0574, -0.1625,  ...,  1.5707,  1.5532,  1.6057],\n",
       "          [ 0.1001,  0.0476, -0.0224,  ...,  1.5707,  1.5882,  1.5882]],\n",
       " \n",
       "         [[-1.3687, -1.3513, -1.3687,  ..., -1.2990, -1.2990, -1.2641],\n",
       "          [-1.3687, -1.3339, -1.3513,  ..., -1.3164, -1.3339, -1.2990],\n",
       "          [-1.3861, -1.3513, -1.3513,  ..., -1.3164, -1.3339, -1.2990],\n",
       "          ...,\n",
       "          [-1.3513, -1.4907, -1.5604,  ..., -1.3513, -1.3513, -1.4036],\n",
       "          [-1.3513, -1.3861, -1.3861,  ..., -1.4384, -1.4210, -1.3687],\n",
       "          [-1.4559, -1.3687, -1.3513,  ..., -1.4036, -1.3513, -1.3513]]]),\n",
       " 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valset = wsds.SequentialDataset(spec, which=\"val\", transformations=partial(make_sample, val=True))\n",
    "next(iter(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imagenet-train-000496.tar  imagenet-train-000569.tar\n"
     ]
    }
   ],
   "source": [
    "!ls ./_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need a sampler for the training set. There are three\n",
    "# special samplers in the `wids` package that work particularly\n",
    "# well with sharded datasets:\n",
    "# - `wids.ShardedSampler` shuffles shards and then samples in shards;\n",
    "#   it guarantees that only one shard is used at a time\n",
    "# - `wids.ChunkedSampler` samples by fixed sized chunks, shuffles\n",
    "#   the chunks, and the the samples within each chunk\n",
    "# - `wids.DistributedChunkedSampler` is like `ChunkedSampler` but\n",
    "#   works with distributed training (it first divides the entire\n",
    "#   dataset into per-node chunks, then the per-node chunks into\n",
    "#   smaller chunks, then shuffles the smaller chunks)\n",
    "\n",
    "# trainsampler = wids.ShardedSampler(trainset)\n",
    "# trainsampler = wids.ChunkedSampler(trainset, chunksize=1000, shuffle=True)\n",
    "trainsampler = wids.DistributedChunkedSampler(trainset, chunksize=1000, shuffle=True)\n",
    "\n",
    "plt.plot(list(trainsampler)[:2500])\n",
    "\n",
    "# Note that the sampler shuffles within each shard before moving on to\n",
    "# the next shard. Furthermore, on the first epoch, the sampler\n",
    "# uses the shards in order, but on subsequent epochs, it shuffles\n",
    "# them. This makes testing and debugging easier. If you don't like\n",
    "# this behavior, you can use shufflefirst=True\n",
    "\n",
    "trainsampler.set_epoch(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for the training and validation datasets\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, num_workers=4, sampler=trainsampler)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "images, classes = next(iter(trainloader))\n",
    "print(images.shape, classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usual PyTorch model definition. We use an uninitialized ResNet50 model.\n",
    "\n",
    "model = resnet50(pretrained=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = deque(maxlen=100), deque(maxlen=100)\n",
    "\n",
    "steps = 0\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    for i, data, verbose in enumerate_report(trainloader, 5):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = outputs.cpu().detach().argmax(dim=1, keepdim=True)\n",
    "        correct = pred.eq(labels.cpu().view_as(pred)).sum().item()\n",
    "        accuracy = correct / float(len(labels))\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(accuracy)\n",
    "        steps += len(labels)\n",
    "\n",
    "        if verbose and len(losses) > 5:\n",
    "            print(\n",
    "                \"[%d, %5d] loss: %.5f correct: %.5f\"\n",
    "                % (epoch + 1, i + 1, np.mean(losses), np.mean(accuracies))\n",
    "            )\n",
    "            running_loss = 0.0\n",
    "\n",
    "        if steps > max_steps:\n",
    "            break\n",
    "    if steps > max_steps:\n",
    "        break\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
