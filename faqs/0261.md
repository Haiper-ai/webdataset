Q: Why is my WebDataset tar file unexpectedly large when saving individual tensors?

A: The large file size is due to the fact that each tensor is pointing to a large underlying byte array buffer, which is being saved in its entirety. This results in saving much more data than just the tensor's contents. To fix this, you should clone the tensor before saving it to ensure that only the relevant data is written to the file. Additionally, each file in a tar archive has a 512-byte header, which can add significant overhead when saving many small files. To reduce file size, consider compressing the tar file or batching tensors before saving.

Here's a code snippet showing how to clone the tensor before saving:

```python
with wds.TarWriter(f"/tmp/dest.tar") as sink:
    for i, d in tqdm(enumerate(tensordict), total=N):
        obj = {"__key__": f"{i}"}
        for k, v in d.items():
            buffer = io.BytesIO()
            torch.save(v.clone(), buffer)  # Clone the tensor here
            obj[f"{k}.pth"] = buffer.getvalue()
        sink.write(obj)
```

To compress the tar file, simply save it with a `.tar.gz` extension and use a compression library:

```python
with wds.TarWriter(f"/tmp/dest.tar.gz", compressor="gz") as sink:
    # ... rest of the code ...
```
