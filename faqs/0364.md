Q: How can I ensure that each validation sample is seen exactly once per epoch in a multi-node setup using WebDataset with FSDP?

A: When using WebDataset in a multi-node setup with Fully Sharded Data Parallel (FSDP), you can ensure that each validation sample is seen exactly once per epoch by assigning each shard to a specific GPU. Since you have an equal number of shards and GPUs, you can map each shard to a GPU. For the shard that is about half the size, you can either accept that the corresponding GPU will do less work, or you can split another shard to balance the load. To ensure that each sample is loaded exactly once, you can use the `wds.ResampledShards` function without resampling, and avoid using `ddp_equalize` since it is designed for training rather than validation. Here's an example of how you might set up your validation dataset:

```py
val_dataset = wds.DataPipeline(
    wds.ResampledShards(
        os.path.join('path', 'to',  'val_samples_{0000...xxxx}.tar')
    ),
    wds.tarfile_to_samples(),
    wds.decode(),
    wds.to_tuple("input.npy", "target.npy"),
    wds.batched(1)
).with_length(num_val_samples)
```

To ensure that the validation loop stops after all samples have been loaded, you can use the length of the dataset to control the number of iterations in your validation loop. This way, you can manually iterate over the dataset and stop when you've reached the total number of samples.
